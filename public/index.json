[{"content":"Since I\u0026rsquo;ve always been interested in game development, I tend to use it as motivation to teach myself new things. This project details my dive into Three.Js and WebGL to create a simple infite flying game.\nBackgound #While doing some research for my thesis project, which you can find here, I came across Karim Maaloul\u0026rsquo;s tutorial on animating a basic 3D scene using Three.js.\nMy Javascript skills were a bit rusty at this point, but I was very familiar with OpenGL and diving into Three.Js, a Javascript library that enables easier functionality for WebGL, seemed easy enough.\nSetting Up #","date":"30 December 2024","permalink":"/projects/aviator-threejs/","section":"Projects","summary":"This project showcases the creation of a simple web game using ThreeJS.","title":"Creating a Game using ThreeJS"},{"content":"","date":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects"},{"content":"Developing a custom game and its engine from scratch.\nThis is a very summarised overview of my MSc Thesis Project, done at the University of Dundee. It focuses on the creation and development of a Doom-like 3D FPS game, and its underlying engine, completely from first principles. It utilises simple, open-source C++ libraries to support some features, but every other aspect was done manually by the game code.\nSince my thesis was over 17 000 words, this project post is not meant to be a deep dive into how I solved or implemented some of the game features. If you\u0026rsquo;re interested in reading it (it\u0026rsquo;s a good read!), shoot me an email.\nCheck out the project code here!\nBackground #I had a couple projects that I was interested in doing for my thesis project:\nA Spacecraft Mission Visualiser program, for S.T.A.R Dundee\u0026rsquo;s PANGU project. A Computer Vision System for Soil Evaluation (the old geoscientist in me was screaming \u0026ldquo;Put me in chief!\u0026rdquo;) Create a 3D Game, and its underlying engine code. I ultimately decided to go with the game engine development for the following reasons:\nIt was something I\u0026rsquo;d be excited to develop, and therefore avoid falling into the trap that is lack of motivation. It offered me the opportunity to learn about graphics programming, specifically about OpenGL. It was a really cool idea. I mean, I\u0026rsquo;m making a game that people could later play, and enjoy. EXTREMELY DOPE. However, I severely underestimated the sheer amount of work required to get a good game going. My supervisor, Dr. Iain Martin, and I set out a goal of getting the core game done within 8-9 weeks, to allow enough time for revision, modification and other improvements. Since this was my first foray into learning both graphics programming and game development, without the use of a commercial engine, I ended up working maybe 12-15 hours a day, learning as I went along.\nThe first couple weeks was spent getting up to speed with understanding OpenGL and the graphics pipeline. Some fantastic resources I used were:\nLearnOpenGL OpenGL-tutorial FreeCodeCamp\u0026rsquo;s OpenGL Tutorials by Victor Gordon Specifications #The game is a 3D Dungeon Crawler First Person Shooter, inspired by Doom. It, and it\u0026rsquo;s engine, which I titled \u0026ldquo;Shadow\u0026rdquo; (just a working title that ended up sticking), was entirely coded using C++ 17 and GLSL (OpenGL Shading Language). Several other C++ libraries were used for different purposes, as detailed below.\nLibrary Usage GLAD OpenGL library that manages functions and pointers for OpenGL, allowing for flexible cross-platform development. GLFW OpenGL library that manages windowing and input/output handling for the engine implementation. GLM OpenGL Mathematics library that provides classes and functions that operate with the OpenGL framework. Used to support matrix, vector and shader calculations. ImGui Lightweight C++ graphical user interface (GUI) library that operates within the 3D graphics pipeline. It outputs interfaces via vertex buffers, allowing for fast rendering implementations. irrKlang Cross platform sound library that manages sound playback throughout the game engine implementation. stb Lightweight, single file C++ library used to load image data, in this case textures, into our game implementation. tmxlite Lightweight C++ library that enables easy parsing of data from the .tmx files that were used to configure level data. Additionally, the Tiled Map Editor was used to create the .tmx files, which holds the layout data necessary for generating level terrain (walls, ceilings and floors), their corresponding textures, enemy spawns and item locations. It allows for easy modification and reconfiguration of the level design, and facilitates easy addition of new textures and enemy types.\nFeatures #The following are some of the more complex features I integrated into the game system.\nOptimised Level Creation #The game\u0026rsquo;s levels were generated using the Tiled Map Editor as a tool to allow easy modification. Tiled stores the layout data in essentially what is a 2D integer array of x-z positions in different object layers. I used this, along with the humble cube, to generate the 3D levels.\n\u0026lt;layer id=\u0026#34;1\u0026#34; name=\u0026#34;Walls\u0026#34; width=\u0026#34;40\u0026#34; height=\u0026#34;30\u0026#34;\u0026gt; \u0026lt;data encoding=\u0026#34;csv\u0026#34;\u0026gt; 0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1, 1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1, 1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1, 1,1,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1, \u0026lt;/data\u0026gt; Each occurrence of the number 1 in the above 40 x 30 array indicates the presence of a cube, at different x-z locations, with a texture ID of 1. Therefore, to generate walls, we could stack these cubes right next to each other, which eliminates the need to create and calculate the individual 3D vertices for each face orientation (left, right, forward, backward, up, down).\nTo address the removal of unneeded geometry, that would be invisible to the player, I created a system using OpenGL\u0026rsquo;s vertex winding order. In OpenGL, triangle primitives have two faces, a front face and a back face. The face type is determined by the order of the three vertices that make up the triangle, and by default, the front face is determined if the winding order of the vertices are counterclockwise.\nOpenGL uses winding orders to define front and back faces of surfaces.\rI then had an algorithm check which faces of the cube would be obstructed by neighbouring terrain, i.e. if a cube face was side by side to another cube face, then we didn\u0026rsquo;t need to render either since we couldn\u0026rsquo;t see them. If there was no obstruction, the algorithm adds the vertices of the unblocked faces, along with their corresponding texture IDs, were then added into a mesh before being passed to the vertex buffer.\nCalculating the vertices to be added to the terrain mesh.\rThis meant that the entire level could be created with a single draw call, massively improving runtime performance and efficiency.\nEnemy Factory #Essentially a combination between two design patterns, the Factory Design Pattern and the Decorator Design Pattern.\nA base enemy class was defined, along with its parameters and functionalities that all enemy instances will share. A particular enemy type is then created using the Factory Design Pattern, which then decorates the base enemy class with specific logic based on the enemy type (eg, zombie logic vs ogre logic.)\nUML Diagram for the Enemy Factory setup.\rAnimation via Texture Cycling #Animation in modern games is usually handled via a process called rigging, where a 3D model of an entity is created and then manipulated via some movement to its skeletal frame. This was unfortunately not possible given the time constraints for development.\nFear not! I still managed to animate enemies via a process I called texture cycling. Think about how old school cartoons or stop motion photography. Textures were rendered onto a simple quad, and then cycled through a preset sequence, at pre-defined intervals, to create animations.\nSpritesheet of an animated zombie attack.\rThe attack, animated in practice.\rThe above example was created using Blender, with a camera set directly in front of the model. At predefined intervals, the model\u0026rsquo;s animation was played, and Blender captured images of the model at that point.\nBillboarding Sprites #Since the enemies and other objects were rendered onto 2D quads, I utilised billboarding to give what were 2D objects the iillusion that they exist in 3D space.\nThis was achieved by rotating the object\u0026rsquo;s quad to always face the inverse direction of the player\u0026rsquo;s point of view (the camera\u0026rsquo;s front vector.) The quad\u0026rsquo;s normal vectors were also dynamically recalculated to always ensure proper lighting calculations and effects.\nDynamic Lighting Effects #Usually, techniques such as normal mapping, shadow mapping and ambient occlusion are commonly utilised to improve graphical fidelity.\nDue to time constraints, I chose to:\nreduce ambient lighting to give the game a dark, \u0026ldquo;spooky\u0026rdquo; feel. Used the Phong lighting model to give the player a spotlight that illuminates a small section of the screen. This serves as the main exploration tool. Set the lighting matrices to dynamically update based on the player\u0026rsquo;s location and direction vector. Therefore the player is able to \u0026ldquo;cast a light\u0026rdquo; on all surfaces, regardless of their position and direction. The spotlight was also attenuated (has a distance falloff) using a quadratic model. shaderprogram-\u0026gt;SetVec3(\u0026#34;spotlight.position\u0026#34;, m_player-\u0026gt;GetPosition()); shaderprogram-\u0026gt;SetVec3(\u0026#34;spotlight.direction\u0026#34;, m_player-\u0026gt;GetFront()); shaderprogram-\u0026gt;SetVec3(\u0026#34;spotlight.ambient\u0026#34;, glm::vec3(0.1f, 0.1f, 0.1f)); shaderprogram-\u0026gt;SetVec3(\u0026#34;spotlight.diffuse\u0026#34;, glm::vec3(0.8f, 0.8f, 0.8f)); shaderprogram-\u0026gt;SetFloat(\u0026#34;spotlight.constant\u0026#34;, 1.0f); shaderprogram-\u0026gt;SetFloat(\u0026#34;spotlight.linear\u0026#34;, 0.09f); shaderprogram-\u0026gt;SetFloat(\u0026#34;spotlight.quadratic\u0026#34;, 0.032f); shaderprogram-\u0026gt;SetFloat(\u0026#34;spotlight.cutOff\u0026#34;, glm::cos(glm::radians(12.5f))); shaderprogram-\u0026gt;SetFloat(\u0026#34;spotlight.outerCutOff\u0026#34;, glm::cos(glm::radians(18.0f))); float distance = length(spotlight.position - FragPos); float attenuation = 1.0 / (spotlight.constant + spotlight.linear * distance + spotlight.quadratic * (distance * distance)); float theta = dot(lightDir, normalize(-spotlight.direction)); float epsilon = spotlight.cutOff - spotlight.outerCutOff; float intensity = clamp((theta - spotlight.outerCutOff) / epsilon, 0.0, 1.0); vec3 ambient = spotlight.ambient * spotlightIntensity; vec3 diffuse = max(dot(norm, lightDir), 0.0) * spotlight.diffuse * spotlightIntensity; vec3 specular = pow(max(dot(viewDir, reflectDir), 0.0), 32) * spotlight.specular * spotlightIntensity; vec3 result = (ambient + (diffuse + specular) * attenuation) * texture(texture1, TexCoords).rgb; FragColor = vec4(result, texture(texture1, TexCoords).a); Breath First Search (BFS) Pathfinding #In order to let the enemies chase after our player character, some type of pathfinding algorithm was needed.\nI chose to utilise a BFS graph traversal algorithm, which iterates over all possibilities between the root node (enemy\u0026rsquo;s current location) and the goal node (player\u0026rsquo;s current location) in order to find the shortest path to the player. While not as efficient in time complexity (it has a linear time efficiency of O(V+E)), it was simple to implement and was more memory efficient when compared to some other algorithms.\nCollision Detection #Collision in games is often handled via Axis Aligned Boundary Box (AABB) collision, due to its calculation efficiency. I defined a bounding box for the player character, and created a system that checks if that box contacts any of the level\u0026rsquo;s terrain meshes.\nIf collision is detected, player movement past the wall’s axis of orientation is stopped, but movement along it is allowed. This ensured two mechanics that significantly improved the fluidity of movement during gameplay:\nThe player contacts the wall but does not get stuck onto it. The player can slide along the wall, meaning that player movement does not suddenly come to a dead stop. ","date":"21 August 2024","permalink":"/projects/game-engine/","section":"Projects","summary":"Developing a custom game and its engine from scratch.","title":"3D Game Engine Development"},{"content":"","date":null,"permalink":"/tags/c++/","section":"Tags","summary":"","title":"C++"},{"content":"","date":null,"permalink":"/tags/game-development/","section":"Tags","summary":"","title":"Game Development"},{"content":"","date":null,"permalink":"/tags/opengl/","section":"Tags","summary":"","title":"OpenGL"},{"content":"As a kid, my favourite games were the Megaman Battle Network and the Megaman Zero iterations of Capcom\u0026rsquo;s hit series. While the story and character of Megaman Zero was amazing (particularly the ending of Megaman Zero 4!), the game mechanics of the Battle Network series is a prime example of introducing novel game mechanics.\nIn the Battle Network series, you\n","date":"21 August 2024","permalink":"/projects/megaman-like/","section":"Projects","summary":"","title":"Reimagining Megaman Mechanics for a modern game"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"\rWelcome to my hub for my projects and musings. I\u0026rsquo;m currently focusing on Machine Learning, Graphics Programming, Game Engine Development and UI/UX.\rHere, you\u0026rsquo;ll find:\nProject Breakdowns: A closer look at my work and current projects. Blog Posts: Short articles on matters that I find interesting. ","date":null,"permalink":"/","section":"","summary":"","title":""},{"content":"About Me #I\u0026rsquo;m a newly minted software developer that loves to blend his creative and technical sides into creating fun products that users enjoy. In my spare time, I\u0026rsquo;m just a chill guy that enjoys weightlifting, photography and cooking.\nHow I Got Here #I was always drawn to science and research, as I was fascinated in understanding the way things just worked. I also wanted to learn as much as I could, and therefore always tried to give myself that opportunity, even if it was difficult to do so. In high school, I elected to study 10 subjects for my GCE O-levels, which was 3 more than everyone else in my year group, and received Distinctions in 9 of them.\nI elected to pursue my Bachelor\u0026rsquo;s degree in Petroleum Geoscience from the University of the West Indies, where I spent the next 3 years learning about the Caribbean\u0026rsquo;s geology and hydrocarbon resources. I graduated with Honours, at the top of my class, and also functioned as President of the local Society of Exploration Geophysicists (SEG) student chapter.\nMapping a sandstone outcrop, Southern Trinidad 2017\rI then spent the next 5+ years as a Geophysicist at my country\u0026rsquo;s Ministry of Energy and Energy Industries, focusing on seismic data analysis and interpretation, data management and policy development. I was able to work on some amazing projects, such as hydrocarbon resource analysis of offshore deepwater environments, and development of a policy framework to guide licensing round evaluations. I also represented the government at many industry events and trade shows.\n2023 Business and Exploration Opportunities Showcase, Islington, London\rAlthough I enjoyed what I was doing, I began suffering from burnout and overwork, likely a leftover effect from the Covid-19 pandemic times. I had become interested in software development during my time as a geophysicist, and had already begun creating very small scale projects for practice.\nI was fortunate to receive a partial scholarship to pursue my Masters in Computer Science at the University of Dundee. I decided to take a leap of faith and shoot for the moon.\nMSc Computer Science Degree Show Day\rMoving to a completely new country to study in a bid to take my career in a new direction was a surreal experience. I\u0026rsquo;m truly grateful that I was able to make that choice for myself, as I met some amazing people and got to learn so many things. I graduated as the Best Overall Student in my MSc cohort, and I\u0026rsquo;m looking forward to things ahead.\nExperience #Education # MSc. Computer Science (Distinction)\nUniversity of Dundee 2023 - 2024 Bsc. Petroleum Geoscience(Honours)\nUniversity of the West Indies 2015 - 2018 ","date":null,"permalink":"/about/","section":"","summary":"","title":"Hi there, I’m Varendra!"},{"content":"Using Convoluted Neural Networks to detect signs of infectious diseases in medical images.\nI\u0026rsquo;m currently redoing this project, based on some improvements shown to me by my classmate, Hannah Gordon. I\u0026rsquo;m also planning to separate the binary classification and multi-class classification into seperate posts for brevity\u0026rsquo;s sake.\rThis project, undertaken as part of Machine Learning Module at the University of Dundee, focuses on using deep learning to classify medical images. It evaluates the performance of the neural network architectures across binary and multi-class classification tasks, showcasing the potential of machine learning in healthcare applications.\nBackground #MedMNIST is a collection of 10 datasets of 28x28 pixel medical images. For this project, two different datasets were chosen, one for binary classification and one for multi-class classification. Each dataset was evaluated using three different networks:\nSample ConvNet : A baseline convolutional neural network provided as part of the coursework. Dense-Only Network : A fully connected network without convolutional layers but with comparable total parameters to the baseline ConvNet. Custom ConvNet : A self-designed deep neural network incorporating advanced design principles, regularization methods, and data augmentation to optimize performance. The networks were built using Google Colab for a couple reasons:\nThe cloud platform meant that there were no incompatibilities between different framework versions. Testing and adjusting network models in the cloud was far easier and didn\u0026rsquo;t require dedicated GPUs. The two chosen datasets were:\nPneumoniaMNIST for binary classification (presence or absence of pneumonia). OrganCMNIST for multi-class classification (predicting organ types). Part 1: Binary Classification of Pneumonia Images #Baseline Convnet #First, I looked at the sample covnet provided by the University, which consisted of the following architecture:\nTwo convolutional layers with ReLU activation and max pooling. A dense output layer with a softmax activation function. Total params: 18379 Trainable params: 18379 from keras import layers from keras import models model_1 = models.Sequential() model_1.add(keras.layers.Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;, input_shape=(28, 28, 1))) model_1.add(keras.layers.MaxPooling2D((2, 2))) model_1.add(keras.layers.Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;)) model_1.add(keras.layers.MaxPooling2D((2, 2))) model_1.add(layers.Flatten()) model_1.add(layers.Dense(11, activation=\u0026#39;softmax\u0026#39;)) model_1.summary() The model was compiled using stochastic gradient descent (SGD) with a learning rate of 0.001 and evaluated using binary cross-entropy loss.\nmodel_1.compile(optimizer=keras.optimizers.SGD(learning_rate=0.001), loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) Binary Base Model Accuracy Plot\rBinary Base Model Loss Plot\rDense Network (Without Convolutions) #To look at the importance of spatial feature extraction, a dense-only network was also tested. This network replaced convolutional layers with fully connected layers, while keeping the total parameter count similar to the ConvNet.\nArchitecture Details:\nInput images were flattened into 1D arrays. Three fully connected layers with 128, 64, and 32 neurons, respectively. Dropout layers were added to prevent overfitting. from keras import layers, models model_dense = models.Sequential() model_dense.add(layers.Flatten(input_shape=(28, 28, 1))) # Flattening the image model_dense.add(layers.Dense(128, activation=\u0026#39;relu\u0026#39;)) model_dense.add(layers.Dropout(0.3)) # Regularization model_dense.add(layers.Dense(64, activation=\u0026#39;relu\u0026#39;)) model_dense.add(layers.Dropout(0.3)) model_dense.add(layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;)) # Sigmoid activation model_dense.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) Binary Dense Model Accuracy Plot\rBinary Dense Model Loss Plot\rChallenges Observed:\nDense networks struggled to capture spatial patterns in the images, resulting in lower performance compared to ConvNets. Dropout regularization improved generalization but did not fully bridge the performance gap. Key Insights:\nDense layers alone lack the ability to identify localized features, highlighting the importance of convolutions for tasks involving image data. Custom Convnet #The custom ConvNet incorporated the following enhancements:\nDepth and Complexity: Increased convolutional layers to capture more detailed features. Regularization: Added dropout layers after each dense layer to prevent overfitting. Batch Normalization: Improved training stability. Data Augmentation: Improved dataset diversity with transformations like rotation, flipping, and zoom. from keras.preprocessing.image import ImageDataGenerator # Data augmentation datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True) # Custom ConvNet model_custom = models.Sequential() model_custom.add(layers.Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;, input_shape=(28, 28, 1))) model_custom.add(layers.BatchNormalization()) model_custom.add(layers.MaxPooling2D((2, 2))) model_custom.add(layers.Dropout(0.3)) # Regularization model_custom.add(layers.Conv2D(64, (3, 3), activation=\u0026#39;relu\u0026#39;)) model_custom.add(layers.BatchNormalization()) model_custom.add(layers.MaxPooling2D((2, 2))) model_custom.add(layers.Dropout(0.4)) model_custom.add(layers.Flatten()) model_custom.add(layers.Dense(128, activation=\u0026#39;relu\u0026#39;)) model_custom.add(layers.Dropout(0.5)) model_custom.add(layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;)) # Sigmoid activation model_custom.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) Binary Custom Model Accuracy Plot\rBinary Custom Model Loss Plot\rResults: The custom ConvNet achieved the best performance among the three models, with significantly improved accuracy and generalization.\nKey Design Takeaways:\nBatch normalization and dropout worked synergistically to stabilize training and reduce overfitting. Data augmentation allowed the model to learn more robust representations. Part 2: Multi-Class classification of Organ Images #Baseline ConvNet #The baseline ConvNet, adapted for multi-class classification, consisted of two convolutional layers followed by a softmax-activated dense output layer for probability distributions.\nTwo convolutional layers with ReLU activation and max-pooling. A dense output layer with softmax activation for multi-class probability distributions. Total parameters: 18,379 model_1 = models.Sequential() model_1.add(layers.Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;, input_shape=(28, 28, 1))) model_1.add(layers.MaxPooling2D((2, 2))) model_1.add(layers.Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;)) model_1.add(layers.MaxPooling2D((2, 2))) model_1.add(layers.Flatten()) model_1.add(layers.Dense(11, activation=\u0026#39;softmax\u0026#39;)) # Softmax for multi-class model_1.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) // Add results here\nDense Network (Without Convolutions) #For multi-class classification, the dense network used three fully connected layers with dropout for regularization.\nThree fully connected layers with 128, 64, and 32 neurons, respectively. Dropout layers for regularization. Softmax activation in the output layer to produce class probabilities. model_dense = models.Sequential() model_dense.add(layers.Flatten(input_shape=(28, 28, 1))) model_dense.add(layers.Dense(128, activation=\u0026#39;relu\u0026#39;)) model_dense.add(layers.Dropout(0.3)) model_dense.add(layers.Dense(64, activation=\u0026#39;relu\u0026#39;)) model_dense.add(layers.Dropout(0.3)) model_dense.add(layers.Dense(11, activation=\u0026#39;softmax\u0026#39;)) # 11 classes model_dense.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) // Add results here\nCustom ConvNet #The custom ConvNet was enhanced for multi-class classification by:\nIncreased Depth: Added a third convolutional layer for deeper feature extraction. Regularization: Introduced higher dropout rates and learning rate scheduling model_custom = models.Sequential() model_custom.add(layers.Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;, input_shape=(28, 28, 1))) model_custom.add(layers.BatchNormalization()) model_custom.add(layers.MaxPooling2D((2, 2))) model_custom.add(layers.Dropout(0.3)) model_custom.add(layers.Conv2D(64, (3, 3), activation=\u0026#39;relu\u0026#39;)) model_custom.add(layers.BatchNormalization()) model_custom.add(layers.MaxPooling2D((2, 2))) model_custom.add(layers.Dropout(0.4)) model_custom.add(layers.Conv2D(128, (3, 3), activation=\u0026#39;relu\u0026#39;)) model_custom.add(layers.BatchNormalization()) model_custom.add(layers.MaxPooling2D((2, 2))) model_custom.add(layers.Dropout(0.5)) model_custom.add(layers.Flatten()) model_custom.add(layers.Dense(256, activation=\u0026#39;relu\u0026#39;)) model_custom.add(layers.Dropout(0.5)) model_custom.add(layers.Dense(11, activation=\u0026#39;softmax\u0026#39;)) model_custom.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) Key Insights\nConvNets Perform Better: Convolutional layers consistently outperformed dense-only networks in image-based tasks. Regularization Works: Dropout, batch normalization, and data augmentation reduced overfitting. Custom Architectures Excel: Tailored designs provided the best performance across both tasks. ","date":"16 March 2024","permalink":"/projects/computer-vision-binary/","section":"Projects","summary":"This project evaluates the performance of neural networks in medical image analysis for disease detection.","title":"Computer Vision using Deep Learning - Disease Detection"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"/tags/chromadb/","section":"Tags","summary":"","title":"Chromadb"},{"content":"","date":null,"permalink":"/tags/chunkig/","section":"Tags","summary":"","title":"Chunkig"},{"content":"","date":null,"permalink":"/tags/gabriel-garcia-marquez/","section":"Tags","summary":"","title":"Gabriel-Garcia-Marquez"},{"content":"","date":null,"permalink":"/tags/langchain/","section":"Tags","summary":"","title":"Langchain"},{"content":"","date":null,"permalink":"/tags/llama3-1/","section":"Tags","summary":"","title":"Llama3-1"},{"content":"","date":null,"permalink":"/tags/llama3-2/","section":"Tags","summary":"","title":"Llama3-2"},{"content":"","date":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"Llm"},{"content":"","date":null,"permalink":"/tags/nomic/","section":"Tags","summary":"","title":"Nomic"},{"content":"","date":null,"permalink":"/tags/ollama/","section":"Tags","summary":"","title":"Ollama"},{"content":"","date":null,"permalink":"/tags/rag/","section":"Tags","summary":"","title":"Rag"},{"content":"","date":null,"permalink":"/tags/retrieval-augmented-generation/","section":"Tags","summary":"","title":"Retrieval-Augmented-Generation"},{"content":"","date":null,"permalink":"/tags/retriever/","section":"Tags","summary":"","title":"Retriever"},{"content":"","date":null,"permalink":"/categories/software/","section":"Categories","summary":"","title":"Software"},{"content":"A web app that combines two APIs to make a useful application\nSee the app here\nAbout #This was my second assignment for my Web Development Module at the University of Dundee. It\u0026rsquo;s written using HTML, CSS and Javascript, with support from JQuery and Material Design Bootstrap. It was my first time working with Javascript and the JSON notation, and was developed over a period of 4 weeks.\nIt uses two API\u0026rsquo;s for its functionality:\nWeather API Ticketmaster API Initially, I looked at all freely available API\u0026rsquo;s from this convenient repository and tried to figure out two things:\nWhat app would I actually be interested in making? What two API\u0026rsquo;s would function well together to make a useful application? I decided on putting together the two aforementioned API\u0026rsquo;s to create an application that allows a user to check both the weather forecast and upcoming events in the area, simply by inputing a designated location. I was also inspired by the Windows 11 Weather app.\nDesign #The initial layout was built based on a basic wireframe design of my concept in Mockflow (Figma was not something I was familiar with, nor had time to get familiar with).\nI initially planned to integrate a geocoding API such as Mapbox API to generate a real-time weather map, but WeatherAPI did not support map tiling so I could not.\nDevelopment #After the initial design was mocked up, I started on developing a base boilerplate website utilising the Material Design Bootstrap (MDB) framework as well as it offered a simple to use, responsive, mobile-first building toolkit. It alos allows every element to scale well across different platforms.\nThe JQuery API was also used to facilitate easy Document Object Model (DOM) traversal.\nI then iterated between using Javascript to fetch API information to create objects that would be inserted into the various sections and then adjusting the HTML and CSS to ensure that the overall framing of the website would stay the same. As mentioned before, the interactive weather map implementation was not feasible due to the Weather API limitations, so the design was subsequently adjusted.\nSince this was my first time working with API data calls and the JSON format, I spent some time getting used to the particular data structures for the different platforms.\nI ran into an issue where the API data for the Weather API was very inconsistent in the dimensions for the icon images for different weather conditions. It essentially contained multiple images of differing sizes, aspect ratio and resolutions. This meant that sometimes they would render incorrectly or be incredibly pixelated. I resolved this by ensuring that all images present in the API were rendered to a set 16:9 ratio.\nI also opted for a very basic colour scheme that had good contrast, which was checked using the colour contrast checker from Coolors.co. Some improvements could be made but, being colourblind myself (deuteranopia), I chose safe colours that I could see well.\n","date":"21 October 2023","permalink":"/projects/api-mashup/","section":"Projects","summary":"'","title":"Weather Event App - API Mashup"},{"content":"","date":null,"permalink":"/tags/web-scraping/","section":"Tags","summary":"","title":"Web-Scraping"}]